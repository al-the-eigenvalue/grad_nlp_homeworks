{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b20f786e",
      "metadata": {
        "id": "b20f786e"
      },
      "source": [
        "# Домашнее задание № 2. Мешок слов"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install nltk razdel pymystem3"
      ],
      "metadata": {
        "id": "hixKESirUTp5"
      },
      "id": "hixKESirUTp5",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import razdel\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from pymystem3 import Mystem\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "Ejh3SrO8NCQT"
      },
      "id": "Ejh3SrO8NCQT",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 42\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)"
      ],
      "metadata": {
        "id": "ilbvO8jDcqEX"
      },
      "id": "ilbvO8jDcqEX",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "russian_stopwords = stopwords.words(\"russian\") + [\"весь\", \"свой\", \"твой\", \"это\", \"очень\", \"год\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ5MutefBm3y",
        "outputId": "1742f849-5ec5-4375-f9e4-c9245be537d1"
      },
      "id": "DQ5MutefBm3y",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mystem = Mystem()"
      ],
      "metadata": {
        "id": "CV4ZNSBiGaWb"
      },
      "id": "CV4ZNSBiGaWb",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"labeled.csv\")\n",
        "train, test = train_test_split(data, test_size=0.1, shuffle=True, random_state=random_seed)\n",
        "train.reset_index(inplace=True)\n",
        "test.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "e7GsNaa2NDlN"
      },
      "id": "e7GsNaa2NDlN",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = train.toxic.values\n",
        "y_test = test.toxic.values"
      ],
      "metadata": {
        "id": "v6giuPEgbynE"
      },
      "id": "v6giuPEgbynE",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0cf72d19",
      "metadata": {
        "id": "0cf72d19"
      },
      "source": [
        "## Задание 1 (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a045e99",
      "metadata": {
        "id": "4a045e99"
      },
      "source": [
        "У векторайзеров в sklearn есть встроенная токенизация на регулярных выражениях. Найдите способо заменить её на кастомную токенизацию"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b4d453",
      "metadata": {
        "id": "90b4d453"
      },
      "source": [
        "Обучите векторайзер с дефолтной токенизацией и с токенизацией razdel.tokenize. Обучите классификатор (любой) с каждым из векторизаторов. Сравните метрики и выберете победителя.\n",
        "\n",
        "(в вашей тетрадке должен быть код обучения и все метрики; если вы сдаете в .py файлах то сохраните полученные метрики в отдельном файле или в комментариях)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TfidfVectorizer с дефолтной токенизацией"
      ],
      "metadata": {
        "id": "2PbUWO-sT3K7"
      },
      "id": "2PbUWO-sT3K7"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1d2477b9",
      "metadata": {
        "id": "1d2477b9"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(min_df=10, max_df=0.3, max_features=100)\n",
        "X = vectorizer.fit_transform(train.comment)\n",
        "X_test = vectorizer.transform(test.comment)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(C=0.1, class_weight=\"balanced\", random_state=random_seed)\n",
        "clf.fit(X, y)\n",
        "preds = clf.predict(X_test)\n",
        "print(classification_report(y_test, preds, zero_division=0, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0XkEJWpSoUA",
        "outputId": "3306ca49-9605-4a3f-bdc8-2e2035c2372d"
      },
      "id": "k0XkEJWpSoUA",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8168    0.7116    0.7606       971\n",
            "         1.0     0.5302    0.6709    0.5923       471\n",
            "\n",
            "    accuracy                         0.6983      1442\n",
            "   macro avg     0.6735    0.6913    0.6765      1442\n",
            "weighted avg     0.7232    0.6983    0.7056      1442\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TfidfVectorizer + razdel.tokenize"
      ],
      "metadata": {
        "id": "WSD7yFyRYTiE"
      },
      "id": "WSD7yFyRYTiE"
    },
    {
      "cell_type": "code",
      "source": [
        "def razdel_tokenize(text):\n",
        "    # возвращаем список токенов, приведенных к нижнему регистру\n",
        "    return [token.text.lower() for token in list(razdel.tokenize(text))]"
      ],
      "metadata": {
        "id": "IWO5grAlYHvd"
      },
      "id": "IWO5grAlYHvd",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=razdel_tokenize,\n",
        "    token_pattern=None,\n",
        "    min_df=10,\n",
        "    max_df=0.3,\n",
        "    max_features=100,\n",
        ")\n",
        "X = vectorizer.fit_transform(train.comment)\n",
        "X_test = vectorizer.transform(test.comment)"
      ],
      "metadata": {
        "id": "euUp5acgWZOO"
      },
      "id": "euUp5acgWZOO",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(C=0.1, class_weight=\"balanced\", random_state=random_seed)\n",
        "clf.fit(X, y)\n",
        "preds = clf.predict(X_test)\n",
        "print(classification_report(y_test, preds, zero_division=0, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G5KwjkraNwR",
        "outputId": "82adcbf3-fc9a-4da8-9997-8479ffad6a52"
      },
      "id": "0G5KwjkraNwR",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8194    0.7147    0.7635       971\n",
            "         1.0     0.5345    0.6752    0.5966       471\n",
            "\n",
            "    accuracy                         0.7018      1442\n",
            "   macro avg     0.6769    0.6949    0.6800      1442\n",
            "weighted avg     0.7263    0.7018    0.7090      1442\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вариант с razdel.tokenize победил: все значения метрик стали выше.\n"
      ],
      "metadata": {
        "id": "m6ez45J4Z-oz"
      },
      "id": "m6ez45J4Z-oz"
    },
    {
      "cell_type": "markdown",
      "id": "91b9076e",
      "metadata": {
        "id": "91b9076e"
      },
      "source": [
        "## Задание 2 (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e25357",
      "metadata": {
        "id": "14e25357"
      },
      "source": [
        "Обучите 2 любых разных классификатора из семинара. Предскажите токсичность для текстов из тестовой выборки (используйте одну и ту же выборку для обоих классификаторов) и найдите 10 самых токсичных для каждого из классификаторов. Сравните получаемые тексты - какие тексты совпадают, какие отличаются, правда ли тексты токсичные?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de962ad",
      "metadata": {
        "id": "7de962ad"
      },
      "source": [
        "Требования к моделям:   \n",
        "а) один классификатор должен использовать CountVectorizer, другой TfidfVectorizer  \n",
        "б) у векторазера должны быть вручную заданы как минимум 5 параметров (можно ставить разные параметры tfidfvectorizer и countvectorizer)  \n",
        "в) у классификатора должно быть задано вручную как минимум 2 параметра (по возможности)  \n",
        "г)  f1 мера каждого из классификаторов должна быть минимум 0.75  \n",
        "\n",
        "*random_seed не считается за параметр"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mystem_lemmatize(text):\n",
        "    mystem_lemmata = []\n",
        "    for token in mystem.analyze(text):\n",
        "        if \"analysis\" in token.keys():\n",
        "            if len(token[\"analysis\"]) > 0:\n",
        "                mystem_lemmata.append(token[\"analysis\"][0][\"lex\"])\n",
        "            else:\n",
        "                mystem_lemmata.append(token[\"text\"].lower())\n",
        "    return mystem_lemmata"
      ],
      "metadata": {
        "id": "eiUB_1yTGCBC"
      },
      "id": "eiUB_1yTGCBC",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_experiment(\n",
        "    vectorizer_type,\n",
        "    classifier_type,\n",
        "    vectorizer_kwargs,\n",
        "    classifier_kwargs,\n",
        "    print_most_toxic_texts=False,\n",
        "    print_most_informative_features=False,\n",
        "):\n",
        "    match vectorizer_type:\n",
        "        case \"count\":\n",
        "            vectorizer = CountVectorizer(**vectorizer_kwargs)\n",
        "        case \"tfidf\":\n",
        "            vectorizer = TfidfVectorizer(**vectorizer_kwargs)\n",
        "        case _:\n",
        "            raise ValueError(\"Invalid vectorizer type\")\n",
        "\n",
        "    match classifier_type:\n",
        "        case \"logreg\":\n",
        "            classifier = LogisticRegression(**classifier_kwargs)\n",
        "        case \"bayes\":\n",
        "            classifier = MultinomialNB(**classifier_kwargs)\n",
        "        case \"tree\":\n",
        "            classifier = DecisionTreeClassifier(**classifier_kwargs)\n",
        "        case \"forest\":\n",
        "            classifier = RandomForestClassifier(**classifier_kwargs)\n",
        "        case _:\n",
        "            raise ValueError(\"Invalid classifier type\")\n",
        "\n",
        "    classifier.random_state = random_seed\n",
        "    X = vectorizer.fit_transform(train.comment)\n",
        "    X_test = vectorizer.transform(test.comment)\n",
        "    classifier.fit(X, y)\n",
        "    preds = classifier.predict(X_test)\n",
        "    print(classification_report(y_test, preds, zero_division=0, digits=4))\n",
        "\n",
        "    if print_most_toxic_texts:\n",
        "        probas = classifier.predict_proba(X_test)[:, 1]\n",
        "        results = pd.DataFrame(\n",
        "            {\"sentence\": test.comment, \"proba\": probas, \"true_label\": y_test}\n",
        "        )\n",
        "        top_n = results.nlargest(10, \"proba\").reset_index(drop=True)\n",
        "        print(top_n)\n",
        "        for i, row in top_n.iterrows():\n",
        "            print(f\"\\nText {i}, probability: {row['proba']}\")\n",
        "            print(f\"\\n{row['sentence']}\")\n",
        "\n",
        "    if print_most_informative_features:\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        match classifier_type:\n",
        "            case \"tree\" | \"forest\":\n",
        "                importances = classifier.feature_importances_\n",
        "            case \"logreg\":\n",
        "                importances = classifier.coef_[0]\n",
        "            case \"bayes\":\n",
        "                word_vectors = vectorizer.transform(feature_names)\n",
        "                class_probas = classifier.predict_proba(word_vectors)\n",
        "                importances = np.squeeze(class_probas[:, 1:], axis=1)\n",
        "        indices = np.argsort(importances)[::-1][:5]\n",
        "        top_features = feature_names[indices]\n",
        "        print(\"Top 5 toxic words:\", \", \".join(top_features))"
      ],
      "metadata": {
        "id": "2sDdLrTrfXg-"
      },
      "id": "2sDdLrTrfXg-",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF + Logistic Regression"
      ],
      "metadata": {
        "id": "2ZG9esZSdZ1j"
      },
      "id": "2ZG9esZSdZ1j"
    },
    {
      "cell_type": "code",
      "source": [
        "setup_experiment(\n",
        "    \"tfidf\", \"logreg\",\n",
        "    vectorizer_kwargs={\n",
        "        \"tokenizer\": mystem_lemmatize,\n",
        "        \"token_pattern\": None,\n",
        "        \"stop_words\": russian_stopwords,\n",
        "        \"max_df\": 0.9,\n",
        "        \"min_df\": 0.001,\n",
        "        \"ngram_range\": (1, 2)\n",
        "    },\n",
        "    classifier_kwargs={\n",
        "        \"C\": 0.1,\n",
        "        \"class_weight\": \"balanced\",\n",
        "    },\n",
        "    print_most_toxic_texts=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGyfVDwY-8rH",
        "outputId": "efdfdbe9-44b8-43ac-c7aa-803d8a3512b0"
      },
      "id": "pGyfVDwY-8rH",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8963    0.8898    0.8930       971\n",
            "         1.0     0.7762    0.7877    0.7819       471\n",
            "\n",
            "    accuracy                         0.8564      1442\n",
            "   macro avg     0.8362    0.8387    0.8374      1442\n",
            "weighted avg     0.8570    0.8564    0.8567      1442\n",
            "\n",
            "                                            sentence     proba  true_label\n",
            "0             Когда тред прощания с хохлами будет?\\n  0.914259         1.0\n",
            "1  Скрипт триггернулся на сочетание свиньи русски...  0.867450         1.0\n",
            "2  русня не умеет в готовку ... ведь поджарить хо...  0.861205         1.0\n",
            "3  Пиздец у быдла с пикабу сначала горело от негр...  0.855859         1.0\n",
            "4               Шлак для малолетних дебилов и баб.\\n  0.852958         1.0\n",
            "5     Даже хохлы в гейропу не хотят, бабосы не те?\\n  0.847751         1.0\n",
            "6  пук пук пук пук МАЛОЛЕТНИЙ ДЕБИЛ НАСРАЛЬНЕНОК ...  0.843512         1.0\n",
            "7  Конечно не для русских. Если русский это сразу...  0.836946         1.0\n",
            "8  Скача по их хуям без разбора? Тебя это ебать н...  0.836945         1.0\n",
            "9  Передайте , который меня в чс кинул, что это н...  0.821110         0.0\n",
            "\n",
            "Text 0, probability: 0.9142591821108577\n",
            "\n",
            "Когда тред прощания с хохлами будет?\n",
            "\n",
            "\n",
            "Text 1, probability: 0.8674502363216342\n",
            "\n",
            "Скрипт триггернулся на сочетание свиньи русский .\n",
            "\n",
            "\n",
            "Text 2, probability: 0.8612051544710647\n",
            "\n",
            "русня не умеет в готовку ... ведь поджарить хохла может только хохол\n",
            "\n",
            "\n",
            "Text 3, probability: 0.855858605823285\n",
            "\n",
            "Пиздец у быдла с пикабу сначала горело от негров на нулевой, теперь от скримеров, куда я нахуй попал, ебаные животные это БЭ, ЭТО РАНДОМ СУЧАРА, ТАМ НЕ ДОЛЖНО БЫТЬ ПРАВИЛ, ПОШЕЛ НАХУЙ\n",
            "\n",
            "\n",
            "Text 4, probability: 0.852957614127054\n",
            "\n",
            "Шлак для малолетних дебилов и баб.\n",
            "\n",
            "\n",
            "Text 5, probability: 0.8477506094625322\n",
            "\n",
            "Даже хохлы в гейропу не хотят, бабосы не те?\n",
            "\n",
            "\n",
            "Text 6, probability: 0.8435124156523346\n",
            "\n",
            "пук пук пук пук МАЛОЛЕТНИЙ ДЕБИЛ НАСРАЛЬНЕНОК ОПЯТЬ НАВАЛИЛ ГАВНА В КАМЕНТАХ\n",
            "\n",
            "Text 7, probability: 0.836945800127866\n",
            "\n",
            "Конечно не для русских. Если русский это сразу нацист.\n",
            "\n",
            "\n",
            "Text 8, probability: 0.8369449318603979\n",
            "\n",
            "Скача по их хуям без разбора? Тебя это ебать не должно, говно\n",
            "\n",
            "Text 9, probability: 0.8211101898107451\n",
            "\n",
            "Передайте , который меня в чс кинул, что это не русский портал. То, что тут посты выходят на русском ещё на означает, что он русский. Тут сидят люди с Украины, Беларуси, Казахстана и других стран, в которых разговаривают на русском.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count + Naive Bayes"
      ],
      "metadata": {
        "id": "7kpHgp71dd3F"
      },
      "id": "7kpHgp71dd3F"
    },
    {
      "cell_type": "code",
      "source": [
        "setup_experiment(\n",
        "    \"count\", \"bayes\",\n",
        "    vectorizer_kwargs={\n",
        "        \"tokenizer\": mystem_lemmatize,\n",
        "        \"token_pattern\": None,\n",
        "        \"stop_words\": russian_stopwords,\n",
        "        \"max_df\": 0.9,\n",
        "        \"min_df\": 0.001,\n",
        "        \"ngram_range\": (1, 1)\n",
        "    },\n",
        "    classifier_kwargs={\n",
        "        \"alpha\": 10.0,\n",
        "        \"fit_prior\": False,\n",
        "    },\n",
        "    print_most_toxic_texts=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD4E7UrZbvj3",
        "outputId": "ac2ebcc5-1704-4136-ed53-8b2dfa2c7eff"
      },
      "id": "tD4E7UrZbvj3",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8874    0.8847    0.8860       971\n",
            "         1.0     0.7637    0.7686    0.7661       471\n",
            "\n",
            "    accuracy                         0.8467      1442\n",
            "   macro avg     0.8256    0.8266    0.8261      1442\n",
            "weighted avg     0.8470    0.8467    0.8469      1442\n",
            "\n",
            "                                            sentence     proba  true_label\n",
            "0  В Киеве на вокзале Мен було рок в 19, коли мен...  1.000000         1.0\n",
            "1  Ну давай разберём всё тобой написанное. Бляядь...  1.000000         1.0\n",
            "2  Возьмём как пример Россию, западноевропейские ...  1.000000         0.0\n",
            "3  моча сюда не заходит И как привлекать их внима...  1.000000         1.0\n",
            "4  Создал тут тхреад в b 192441781 Как оказалось,...  1.000000         1.0\n",
            "5  Интересно, а что мы могли сделать? Ввести санк...  1.000000         1.0\n",
            "6  черт опущенный Гомикадзе Би опущенный гей на е...  1.000000         1.0\n",
            "7  Пиздец у быдла с пикабу сначала горело от негр...  0.999999         1.0\n",
            "8  Я всегда говорил, что пидорахи плохо понимают ...  0.999999         1.0\n",
            "9  Любишь ГРОТ-люби и хуй в рот, как грится. Отвр...  0.999998         1.0\n",
            "\n",
            "Text 0, probability: 1.0\n",
            "\n",
            "В Киеве на вокзале Мен було рок в 19, коли мене перший раз мав в зад хлопець рок в п д 30. Я тод перш рази став заходити на вокзал Ки в-Пасажирський в туалети - де були каб ни з д рками написи на ст нах. Так як досв ду ще не було н якого, то як знайомиться не уявляв. Сам перший природно не п дходив. А видивлявся на написи. дрочив св й член стоячи в каб нц . Хлопець був у сус дн й каб нц , в н побачив це, хитнув мен головою, запрошуючи п ти з ним. А так як н кого б льше в той момент не було, а був уже веч р, над на щось нше не було -все ж п шов за ним. У мене вже тод з явилася молофья - я вже спускав. Так як трохи ран ше ще не було, при дрочц робив це до при много стану - коли просто ставало дуже добре - але з хуя н чого не вид лялося. А до цього мен вже к лька раз в смоктали член хлопц мужики, я спускав м в рот, знав як це при мно. Ми прийшли б ля вокзалу кудись в кущ . В н розстебнув мен мотню, д став м й член став дрочити. А в той час нав ть це - коли хтось чужий рукою просто всього лише дрочив мен - було все одно дуже при мно. забирало. Бо коли тоб дрочать чужою рукою в дитинств - це вже щось: в д цього балд ш дуже. В н, ймов рно, здогадуючись, що перед ним зовс м новачок не намагався нав ть мен св й дати в руку: Так в н мене зав в , а пот м попросив повернутися: Я запитав нав що, справд не розум ючи нав що - а в н сказав треба так. я як теля повернувся п дкоряючись команд дорослого. В н приспустив мен штани, труси приставив до дерева у якого ми стояли, трохи нагнувши мене. А сам встав ззаду. По звуках я зрозум в, що в н розст ба соб свою мотню д ста св й член. В н притулився до мо поп сво м хуем, в д чого я здригнувся, але в н взяв мене за м й член знову став дрочити. А ншою рукою водити по стегнах з внутр шньо сторони. П д ймаючись в д кол н до поп - це посилювало кайф в д дрочк , я мл в, в н це теж в дчував, вже спок йно став тертися сво м хуем мен по поп . Пот м в н перестав дрочити мен , я почув як в н послинив св й член мою д рочку приставив мен св й член, в дсунув мене в д дерева трохи, пригнув мене почав засовували член в мене. Я стояв нагнувшись, упершись руками в дерево, н живий, н мертвий - перший раз в житт хлопець в мене засовував св й хуй! Я боявся - як все буде, що буде з мною, як це. Мен пощастило, звичайно, для першого разу, що у нього був маленький тонкий хуй. Тому н яких проблем у нього з всуванням його хуя в мою св жу попку не було. Оск льки мен не було боляче або непри мно я стояв не с паючись. Чекаючи як що буде дал :. В н засунув св й член весь в мене. т льки коли в н встромив його до к нця - було в дчуття що в н у щось уперся. Але не боляче зовс м. треба сказати чесно, що було при мно в дчувати, коли яйця його доторкнулися до мо попки, до д рочки, коли весь член був уже всередин не .. Це при мне в дчуття, коли умоглядно уявля ш що в тебе чийсь член засунуть: Це було мабуть нав ть при мн ше н ж все нше - в дчувати його яйця б ля очка. Коли весь член вже там. коли в н пот м став й бати мене, я намагався щоб част ше яйця його впиралися в попу мен , нав ть нод насаджувався сам глибше на його член, до упору. Але показувати що мен щось при мно тод здавалося ще не зручним - б льший час я просто стояв обхопивши дерево руками, а в н вставляв член в мене. Хоча особливого кайфу я ще тод не в дчував - було в дчуття - що просто в мене встромляв хлопець св й член ходив там. Так в н мав мене, продовжуючи одночасно весь цей час одн ю рукою дрочить мен - п дтримуючи в мен бажання: - ось в д цього мен було при мно. Природно. Це був його розрахунок. Я досить швидко в д дрочк чужою рукою спустив, в дразу з скочив з його члена. Але в дчув що у мене щось липкою ззаду на стегнах: Що щось тече по стегнах з очка. ось це мене засмутило сильно. вбило - я здогадався зрозум в що в н спустив в мене. Запитав, - Ти що ск нчив у мене? в н сказав - так. запитав мене - а ти що перший раз це робив? я мало не плачучи в д образи сказав - що так, перший раз: поставив йому дурне питання - нав що ти в мене спустив? Я не припускав цього, думав що в н просто посует ться в мене св й хуй все, а тут мен стало не по соб : було огидно, - особливо п сля того як сам пустив, - що на мен чужа гидота , як тод сприймав чужу малофю . Та тим б льше на сво му т л . Але справа була зроблена: хлопця 19 рок в видрали в дупу! спустили сперму йому в очко! В струнку, пружну, н жну попочку з н жною д рочкою, засунувши в не перший раз член! У перший момент було огидно в д того що щось липке, спочатку тепле, ст кало по стегнах, а пот м застигло так: (а так як не готувався до цього, то витерти було н чим:) Тод було прикро, не за те що ви бав, а що не попередивши, спустив в мене. Так як тод сперма сприймалася як щось мерзенне, тим б льше на соб . Пот м згадував про це вже з та мним насолодою, нав ть бажанням, щоб це повторилося: я поб г швидко з цього м сця, скор ше в д нього, а липка р дина на стегнах весь час нагадувала, що мене т льки що ви бли в жопу. Слава Укра н !\n",
            "\n",
            "Text 1, probability: 1.0\n",
            "\n",
            "Ну давай разберём всё тобой написанное. Бляядь, вы действительно думаете вы лучше пидорашек? Ну в целом, всё что живёт в рашке - затронуто говномидасом, но никто тут это не признает. сейчас воспитывают массу хороших кодеров В соседнем треде обоссали уже. Иди обтекай. Вы унижаете русских детей в школе Я учился в рашке и у нас был класс, который состоял онли из русачков. Думаешь, что то изменилось? Чурки тебе говна в жопу залили и заставили русачков в классе кошмарить омежных русачков? Я не люблю выражаться фразами нациков, но вы воистину столетия просто сидели в горах и ебали баранов, вас даже народ-пидор смог захватить. Плоховато ты знаешь историю. Когда русня пришла на Кавказ, тут всё уже было поделено османами и персами. А потом РИ наебала персов и постоянно нападала на османов высасывая причины из пальца, в принципе, русачки, что от них ещё было ожидать. В прошлом вы (чеченцы, даги и прочие сорта) были просто дикарями...А если говорить о среднеазиатах, которые бугуртят с оккупации, то вы были обычными нищими кочевниками Очередной акт незнания истории. Например у кавказцев по факту у народов отвественных за этногенез дагов и азеров уже был Дербент, а русачков даже в планах не было. Почитай историю Кавказа и Средней Азии, там были и локальные империи и нагибы округи и прочее и прочее. Называют русских пидорашками, славщитом. Говоришь это так, будто бы в этом что то плохое. У меня знакомые po рашеры irl, которые являются русскими и которые ссут на русню с ещё большей колокольни чем я Сейчас вас все боятся потому, что если пидорашка сделает в вас пару дырок, защищая себя, то его посадят на бутылку, а если вы убьёте пидорашку, то вам нихуя не будет. Может потому что в рашке мочить с волыны ножа человека, который идёт на тебя с кулаками - превышение пределов самообороны? Кто виноват в том, что русня настолько вырожденческая, что один чеченский доходяга ставит на колени группу русачков? Ты думаешь, будь руснявая гопота менее вырожденческая, то так же людей не кошмарила? Кто виноват, что вы морозитесь или даёте по съебатору, когда видите, как вашего славянского друга избивают унижают? Да чего уж там, тот митинг вспомните, где жирик, какого то парня на колени поставил и все вокруг стоят и снимают, словно стадо руснявых баранов. Зуб даю, в той толпе стояли его друзья и знакомые. Лично для меня это пиздец, особенно проигрываю, когда там не какой нибудь братуха-борцуха, а смарчёк чеченский. Никогда бы не бросил и не бросал друга в таком пиздеце и не важно, какой нации был друг, а какой нападавший. Вся ваша проблема не том, что вы слабками, не в том, что большинство русачков еблом похож на свинью, не в том, что за тысячу лет существования не смогли построить нормальную цивилизацию и другим не давали. Проблема в обыкновенной ошибке выжившего, вы видите кавказское быдло гопника, потому что оно в среднем сильнее, напористее, агрессивнее и образ кавказца, как лица которое представляет опасность выжигается у вас в мозгах, при этом не хотите замечать, что у вас, целые города набитые руснявыми АУЕшниками, потому что один среднестатистический руснявый гопник ауешник быдлойд сосёт у одного среднестатистического чуркобесского гопника быдлойда. В конечном итоге русачков в станице кущевской ебал кто? Другие русачки. В школу приезжали и выберали лолей на поёбку кто? Другие русачки. Сжигал русачков кто? Другие русачки. Всё вскрылось совершенно случайно. Сколько таких станиц, деревень и городов по всей рашке? аз-кун\n",
            "\n",
            "\n",
            "Text 2, probability: 1.0\n",
            "\n",
            "Возьмём как пример Россию, западноевропейские страны и США. Идёт метисация, сознательная политика замещения белого населения на пришлое черно-коричневое. Идёт создание новой расы метисов, исламизация и почернение. В крупных городах половина населения - выходцы из ебеней Мексики, Африки, Ближнего Востока, а в случае с Россией - Кавказа и Средней Азии. Этнические ниггеро-арабские гетто верят на хую законы как хотят, чудовищная по масштабам этническая преступность. Говорить о миграции и тем более затрагивать тему замещения коренного населения властями нельзя, иначе бутылка. Свобода слова тут не для вас, молодой человек. При этом говорить о том, что белые должны вымереть, и это нормально - можно. Белые официально вымирают ведётся пропаганда так или иначе направленная на снижение рождаемости белого населения. Феминизм, ЛГБТ, чайлдфри. Каждая женщина в Швеции - леволиберальная феминистка, это страна победившего феминизма. Что сегодня там происходит - страшно делается. Пропагандируются смешанные браки, межрасовые браки, пропагандируется превосходство детей-метисов. Идёт демонизация белых и пропаганда превосходства чёрных и смуглых мужчин, форс отношений белая женщина смуглый чёрный мужчина-мигрант. Как результат - всё больше чернильниц, всё больше смешанных браков, всё больше небелых метисов. Белые женщины просто не хотят контактировать с мужчинами своей нации и расы, наделяя их самыми плохими качествами и обожествляя черных. При этом большинство белых не считает завоз чурок чем-то плохим, наоборот, относятся к ним толерантно. Проводится политика насаждения толерантности, мультикультурализма, политкорректности и космополитизма. Набирающее популярность даже в России SJW - это вообще отдельная тема для обсуждения. Всё вышеперечисленное относится к сильнейшим когда-то странам, бывшим империям, нагибающим слабых. Сегодня происходит так, что бывшие империи в прямом смысле деградируют, вырождаются и вымирают, а место сильнейших когда-то, господствующих народов, занимают те, кого когда-то колонизировали. Во Франции к 2080 уже будут доминировать негры и арабы, в России - кавказцы и выходцы из средней Азии, в Великобритании - индийцы, негры, арабы, пакистанцы, etc. А в маленьких, нейтральных странах, вроде Словении или Беларуси, Литвы или Чехии, Румынии или Эстонии - всё пучком. Им вымирание не грозит, они остаются и будут оставаться белыми. Более того, у них ведётся политика, направленная на сохранение традиционных ценностей и культуры коренного населения. Они сказали беженцам нет . В Польшу, например, русскому или украинцу гораздо легче переехать и остаться, чем арабу или африканцу. В Германии ситуация противоположная, белых там не ждут. Польша, Чехия, Словакия, Венгрия, Словения, Хорватия, Сербия, БиГ, Черногория, Македония, Греция, Болгария, Румыния, Молдова, Украина, Беларусь, Литва, Латвия, Эстония - вот Европа будущего. Скандинавия, Южная, Западная Европа, а также Россия - лишатся коренного населения и своей культуры.\n",
            "\n",
            "\n",
            "Text 3, probability: 0.9999999999605507\n",
            "\n",
            "моча сюда не заходит И как привлекать их внимание? Флешмоб организовать что-ли? Если будешь надоедать моче борьбой с поехавшим, то сам будешь не сильно отличаться от поехавшего. Спейсачерам просто пора научиться игнорировать этого идиота и просто бампать другие треды, вместо того чтобы отвечать шизику. Всё равно никто не хочет с ним говорить, но у него реально в голове иллюзия создается, что он какой-то интересный человек, хотя по сути ему на лицо ссут, а он думает что с ним нормально общаются. Я думаю с ним в жизни никто не общается и даже когда ему на лицо ссут, банят, посылают нахуй, он думает что это общение и радуется как ребенок.\n",
            "\n",
            "\n",
            "Text 4, probability: 0.9999999995678763\n",
            "\n",
            "Создал тут тхреад в b 192441781 Как оказалось, ЛГБТ пропаганда в б кажется унылой и слишком обильной не только мне. Как насчёт пидорнуть все гее би треды из b в ga? Хуле они тут у себя филиал открыли? Заебали притеснять натуралов. Давайте я поясню. Пидоротреды унылы и неприятны, и их крайне много. Это заебало. Если куклоебов и пониебоы пидорнули, почему пидоров нельзя пидорнуть? Пониебы всех заебали - их пидорнули Куклоебы всех заебали - их пидорнули Пидоротреды заебали меня, я интересуюсь, сколько ещё анонов заебались их видить нонстоп 24 7 на главной. Если нас будет много - можно и пидорнуть, я считаю. Для них есть целый раздел, или 2-3 даже. Но все равно постоянно это лезет на главную в b. ДОКОЛЕ?\n",
            "\n",
            "\n",
            "Text 5, probability: 0.9999999109510083\n",
            "\n",
            "Интересно, а что мы могли сделать? Ввести санкции на поставки гуманитарки для своего населения? Алсо, не примешивай к этому Пыню, Пыня проебал украину имея проросийского президента, яиц хватило только на то, чтобы откусить под шумок крым, чем обеспечить абсолютную ненависть хохлов в остальной украине и страх восточноевропейских карликов, которые теперь готовы в жопу дать за защиту от русской угрозы .\n",
            "\n",
            "\n",
            "Text 6, probability: 0.999999858827205\n",
            "\n",
            "черт опущенный Гомикадзе Би опущенный гей на ебало ссали хуй сосать черт и пиздабол чисто мразь в мазке. Одна ложь и грязь. пиздаболище ПиськаМазе х15р 150р Угомонись, разоришь пригожина\n",
            "\n",
            "\n",
            "Text 7, probability: 0.999999496887908\n",
            "\n",
            "Пиздец у быдла с пикабу сначала горело от негров на нулевой, теперь от скримеров, куда я нахуй попал, ебаные животные это БЭ, ЭТО РАНДОМ СУЧАРА, ТАМ НЕ ДОЛЖНО БЫТЬ ПРАВИЛ, ПОШЕЛ НАХУЙ\n",
            "\n",
            "\n",
            "Text 8, probability: 0.9999994200767025\n",
            "\n",
            "Я всегда говорил, что пидорахи плохо понимают как выглядят типо европейские лица, на которые русня дрочат. привыкши жить среди орков со светлыми волосами и глазами они воспринимают только таких орков с приплюснутой гловой и свинной рожей как своих, если у кого либо европейское лицо, развитая челюсть и т.п но темные волосы, то к нему относятся с подозрением Баба эта похожа на марлона брандо.\n",
            "\n",
            "\n",
            "Text 9, probability: 0.999997911586582\n",
            "\n",
            "Любишь ГРОТ-люби и хуй в рот, как грится. Отвратительные пиздострадальные лицемеры, что прикрываются идеями о русской нации и о том, что политики пидорасы и вообще плохие, а потом классненько выступают на политических штучках организованных ЕДРОм. Я не против зарабатывания кэша, но тогда не надо при этом лицемерить.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Общий текст только один (топ 4 у TF-IDF + logreg и топ 8 у Count + Bayes).\n",
        "\n",
        "У каждого эксперимента все топ 10 текстов токсичные, кроме одного.\n",
        "\n",
        "У TF-IDF + logreg неправильно определенный текст - это топ 10: в нем 4 раза встретилось слово \"русский\", которое попало в топ 5 токсичных слов у 3 из 4 классификаторов (см. задание 4).\n",
        "\n",
        "У Count + Bayes \"неправильно\" определенный текст - это топ 3. Он однозначно токсичный, здесь ошибка разметки."
      ],
      "metadata": {
        "id": "YxGiUMjpqqX6"
      },
      "id": "YxGiUMjpqqX6"
    },
    {
      "cell_type": "markdown",
      "id": "6f228c3e",
      "metadata": {
        "id": "6f228c3e"
      },
      "source": [
        "## Задание 3 (4 балла - 1 балл за каждый классификатор)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "566929b7",
      "metadata": {
        "id": "566929b7"
      },
      "source": [
        "Для классификаторов Logistic Regression, Decision Trees, Naive Bayes, RandomForest найдите способ извлечь важность признаков для предсказания токсичного класса. Сопоставьте полученные числа со словами (или нграммами) в словаре и найдите топ - 5 \"токсичных\" слов для каждого из классификаторов.\n",
        "\n",
        "Важное требование: в топе не должно быть стоп-слов. Для этого вам нужно будет правильным образом настроить векторизацию.\n",
        "Также как и в предыдущем задании у классификаторов должно быть задано вручную как минимум 2 параметра (по возможности, f1 мера каждого из классификаторов должна быть минимум 0.75"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Извлечение важности признаков - параметр ```print_most_informative_features``` в функции ```setup_experiment```.\n",
        "\n",
        "Для ```MultinomialNB```: ```classifier.feature_log_prob_``` — это вероятность признака (слова) при условии конкретного класса. Это недостаточно информативно, потому что частые слова будут в топе ```classifier.feature_log_prob_``` у обоих классов. В качестве метрики информативности признака более наглядна вероятность класса при условии наличия конкретного признака (слова). Для этого мы берем все признаки:\n",
        "\n",
        "```word_vectors = vectorizer.transform(feature_names)```\n",
        "\n",
        "Считаем вероятности классов при условии каждого индивидуального признака:\n",
        "\n",
        "```class_probas = classifier.predict_proba(word_vectors)```\n",
        "\n",
        "Информативность (важность признака) для класса 1 - это вероятность класса 1:\n",
        "\n",
        "```importances = np.squeeze(class_probs[:, 1:], axis=1)```"
      ],
      "metadata": {
        "id": "u6L9QELvmBNE"
      },
      "id": "u6L9QELvmBNE"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "81f86878",
      "metadata": {
        "id": "81f86878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c9cea1-4353-4491-ff07-f12510609631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8963    0.8898    0.8930       971\n",
            "         1.0     0.7762    0.7877    0.7819       471\n",
            "\n",
            "    accuracy                         0.8564      1442\n",
            "   macro avg     0.8362    0.8387    0.8374      1442\n",
            "weighted avg     0.8570    0.8564    0.8567      1442\n",
            "\n",
            "Top 5 toxic words: хохол, русский, тупой, хохлов, дебил\n"
          ]
        }
      ],
      "source": [
        "setup_experiment(\n",
        "    \"tfidf\", \"logreg\",\n",
        "    vectorizer_kwargs={\n",
        "        \"tokenizer\": mystem_lemmatize,\n",
        "        \"token_pattern\": None,\n",
        "        \"stop_words\": russian_stopwords,\n",
        "        \"max_df\": 0.9,\n",
        "        \"min_df\": 0.001,\n",
        "        \"ngram_range\": (1, 2)\n",
        "    },\n",
        "    classifier_kwargs={\n",
        "        \"C\": 0.1,\n",
        "        \"class_weight\": \"balanced\",\n",
        "    },\n",
        "    print_most_informative_features=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "setup_experiment(\n",
        "    \"count\", \"bayes\",\n",
        "    vectorizer_kwargs={\n",
        "        \"tokenizer\": mystem_lemmatize,\n",
        "        \"token_pattern\": None,\n",
        "        \"stop_words\": russian_stopwords,\n",
        "        \"max_df\": 0.9,\n",
        "        \"min_df\": 0.001,\n",
        "        \"ngram_range\": (1, 1)\n",
        "    },\n",
        "    classifier_kwargs={\n",
        "        \"alpha\": 10.0,\n",
        "        \"fit_prior\": False,\n",
        "    },\n",
        "    print_most_informative_features=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrRfeeIeUFRt",
        "outputId": "ef8fadf6-967d-4ab2-c59d-46ff7e726125"
      },
      "id": "lrRfeeIeUFRt",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8874    0.8847    0.8860       971\n",
            "         1.0     0.7637    0.7686    0.7661       471\n",
            "\n",
            "    accuracy                         0.8467      1442\n",
            "   macro avg     0.8256    0.8266    0.8261      1442\n",
            "weighted avg     0.8470    0.8467    0.8469      1442\n",
            "\n",
            "Top 5 toxic words: хохол, хохлов, дебил, дегенерат, шлюха\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "setup_experiment(\n",
        "    \"count\", \"tree\",\n",
        "    vectorizer_kwargs={\n",
        "        \"tokenizer\": mystem_lemmatize,\n",
        "        \"token_pattern\": None,\n",
        "        \"stop_words\": russian_stopwords,\n",
        "        \"max_df\": 0.9,\n",
        "        \"min_df\": 0.001,\n",
        "        \"ngram_range\": (1, 1)\n",
        "    },\n",
        "    classifier_kwargs={\n",
        "        \"min_samples_split\": 600,\n",
        "        \"criterion\": \"log_loss\",\n",
        "    },\n",
        "    print_most_informative_features=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4uABqVyTs3F",
        "outputId": "38e7ba7d-229c-4856-cd5e-9bd3654e4c9c"
      },
      "id": "a4uABqVyTs3F",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8685    0.8568    0.8626       971\n",
            "         1.0     0.7128    0.7325    0.7225       471\n",
            "\n",
            "    accuracy                         0.8162      1442\n",
            "   macro avg     0.7906    0.7947    0.7926      1442\n",
            "weighted avg     0.8176    0.8162    0.8169      1442\n",
            "\n",
            "Top 5 toxic words: хохол, тупой, русский, хохлов, ебать\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "setup_experiment(\n",
        "    \"count\", \"forest\",\n",
        "    vectorizer_kwargs={\n",
        "        \"tokenizer\": mystem_lemmatize,\n",
        "        \"token_pattern\": None,\n",
        "        \"stop_words\": russian_stopwords,\n",
        "        \"max_df\": 0.9,\n",
        "        \"min_df\": 0.001,\n",
        "        \"ngram_range\": (1, 1)\n",
        "    },\n",
        "    classifier_kwargs={\n",
        "        \"min_samples_split\": 600,\n",
        "        \"criterion\": \"log_loss\",\n",
        "        \"n_estimators\": 200,\n",
        "    },\n",
        "    print_most_informative_features=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-f-bFfHYCuy",
        "outputId": "798278aa-325b-4ac2-decb-0d6f101f99b2"
      },
      "id": "X-f-bFfHYCuy",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8673    0.9156    0.8908       971\n",
            "         1.0     0.8034    0.7113    0.7545       471\n",
            "\n",
            "    accuracy                         0.8488      1442\n",
            "   macro avg     0.8353    0.8134    0.8226      1442\n",
            "weighted avg     0.8464    0.8488    0.8463      1442\n",
            "\n",
            "Top 5 toxic words: хохол, русский, хохлов, тупой, дебил\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}